\chapter{Experimental setup}
% \section{Flower.ai first draft}
% Flower.ai\footnote{\url{https://flower.ai/}} is a Python library that allows for the distributed training of machine learning models. It is designed to be framework-agnostic and can be used with any machine learning framework that supports the Python programming language, such as TensorFlow, PyTorch, and scikit-learn. Flower.ai provides a simple and intuitive API for distributed training, allowing users to easily scale their machine learning models across multiple devices and machines. However, in its current version Flower.ai enforces developpers to follow a strict pipeline for the training process. While implementing the MD-GAN, we found that the Flower.ai library was not flexible enough to accommodate the specific requirements of the MD-GAN architecture. We therefore decided to implement it using PyTorch's distributed package, which provides more control over the training process and allows for a more customized implementation.

\section{PyTorch distributed package}
PyTorch's distributed package c10d\footnote{\url{https://pytorch.org/docs/stable/distributed.html}} allows for sending and receiving tensors across various processes, devices, and machines. It operates at a low level, achieving great performance while providing the user with considerable flexibility. Since we are already very familiar with the PyTorch library, it was a natural choice for us.


We used a TCP backend within this framework to enable strong network communication between machines over the network. This setup allows us to efficiently send feedback to the central server and distribute generated images to the workers, facilitating collaborative training of the GAN across multiple nodes. However, we cannot send dictionaries or complex objects, which are essential for the MD-GAN architecture, as we need to exchange the state of discriminators to avoid overfitting. Therefore, we decided to use TensorDict\footnote{\url{https://github.com/pytorch/tensordict}} \cite{tensordict}, a module integrated into PyTorch, that turns dictionaries into tensors and vice versa. It is also capable of sending and receiving these tensors across different processes using the same TCP backend, fitting perfectly with our requirements.

\subsection{Backend choice}
PyTorch's distributed package offers two main options, GLOO and NCCL, each with its own advantages and caveats. The primary difference lies in the location of the tensors being sent and received. With the NCCL backend, communication occurs directly through an NVIDIA device, allowing for better performance. In contrast, GLOO passes through the CPU, offering greater flexibility.

However, NCCL does not support having multiple nodes running on one GPU, which goes against our architecture as shown in figure \ref{fig:architecture}. To take full advantage of the computational power of our instances, we run multiple workers on a single machine. Therefore, we chose to use the GLOO backend, offering us more flexibility in how many workers we instantiate on a Compute Engine. By doing so, since the tensors are processed on the GPU in our experimental setup, we have to move them to the CPU to allow for sending and receiving data. This introduces some latency as messages are transferred back to the GPU for processing, but this trade-off is justified by the flexibility it offers.

In cases where only a single worker lies in each instance, our novel swapping strategy makes it possible to use NCCL which would boost the performance.

% \section{Metrics used}
% To evaluate the performance and quality of the data generated by our distributed GAN, we utilized TorchMetrics\footnote{\url{https://lightning.ai/docs/torchmetrics/stable/}}, a library developed by LightningAI. This package includes over 100 implemented PyTorch metrics, significantly simplifying their integration into our model. As discussed previously, we employed two key metrics: the Frechet Inception Distance (FID) and the Inception Score (IS).


\section{Baseline Comparison}
To evaluate the performance of our distributed MD-GAN implementation, we compared it to a standalone centralized implementation. This baseline runs on a single machine with a single NVIDIA T4 GPU to train the GAN on the entire dataset. This setup serves as a reference point, allowing us to measure the benefits of distributed training and the impact of spreading the training process across multiple machines. We used the same hyperparameters and training settings for both the distributed and centralized implementations to ensure a fair comparison. It is important to note that the centralized implementation requires all data to be on one machine, meaning the data must be owned by the machine running the training process. This is not the case for the distributed implementation.

\section{Datasets used} \label{sec:datasets}
To evaluate our distributed Multi-Discriminator GAN implementation, we used three datasets: MNIST \cite{mnist}, CelebA \cite{celeba} and CIFAR-10 \cite{cifar}.

\begin{itemize}
    \item The MNIST dataset consists of 60,000 grayscale images of handwritten digits with 10 classes (0-9). Each image is 28x28 pixels in size, summing up to 784 input features, it is a relatively small and simple dataset.
    \item CelebA consists of 202,599 colored image of celebrity faces, each of size 178x218 and which sums up to 116,412 input features (including 3 channels), to reduce the input size of our model we resize every image to 64x64 and preserve the 3 channels, ending up with 12,288 input parameters instead.
    \item The CIFAR-10 dataset consists of 60,000 32x32 colored images which sums up to 3,072 input features (including 3 channels), in 10 classes, with 6,000 images per class.
\end{itemize}

Even if CelebA contains more input parameters it is more conceptually simple than CIFAR-10, due to that fact that every image consists of single human face, the angle could vary but the subject is still the same. In the other hand CIFAR-10 contains very different classes, making CIFAR-10 the most complex data distribution to learn.

We believe that these three datasets are a great choice to accurately evaluate our implementation, they are widely used in the literature and cover various type of classes and use cases, especially in the computer vision field and generative models.

In all our experiments, we distributed the data in an IID (Independent and Identically Distributed) manner across the workers. However, it is possible to distribute the data non-IID (referring to \ref{appendix:args}), which could produce different results. Although this feature is implemented, we chose not to include it in our study due to time constraints.

% By using these three datasets, we can evaluate the performance of our distributed MD-GAN implementation on both simple and complex data distributions, providing a comprehensive assessment of its effectiveness across different scenarios. We choosed these datasets because they were the ones used in the original MD-GAN paper, so we can compare our results with the ones obtained by the authors.

% \section{Local setup}
% It is possible to run the distributed MD-GAN implementation on a single machine because each worker runs as a separate Python process. This setup allows us to run multiple workers on the same machine. To facilitate this, we provide a script called \code{run-distributed.sh} which helps to start the server and workers. This bootstrap script allows anyone to run a few workers locally or in a distributed setting. One simply needs to set the parameters of the experiment in the \code{set-args.sh} script and then use \code{run-distributed.sh} to run the experiment and indicate the workers ranks while calling the script.

% Every run will collect statistics and log them in CSV file. These files can be interpreted exploited using a Juypter Notebook \code{plot-logs.ipynb} which will generate all the plots in included in this report.

\section{Models}
For each dataset in our experiment, we need to define a generator model, a discriminator model, a class to load the dataset (Partitioner), and some global properties of the dataset. To do this, we created an application architecture that makes it easy to add new datasets. The \code{datasets} folder should have one file for each dataset, and each file must include the following three classes and two constants:
\begin{itemize}
    \item \code{Partitioner}: This class is responsible for loading the dataset and providing methods to perform operations such as partitioning or selecting a subset of data.
    \item \code{Discriminator}: This class defines the architecture of the discriminator for the dataset.
    \item \code{Generator}: This class defines the architecture of the generator for the dataset.
    \item \code{SHAPE}: This global constant specifies the shape of the data that the server will use to generate fake batches for the workers. It should be a tuple of three integers (\code{Tuple[int, int, int]}) representing the number of channels, the width in pixels, and the height in pixels.
    \item \code{Z\_DIM}: This constant specifies the dimension of the input noise that the generator will use to generate a new image.
\end{itemize}

We have established all the necessary components for evaluating MNIST, CIFAR-10, and CelebA. To find suitable model architectures adapted for each dataset, we opted for a DCGAN\footnote{A DCGAN is a GAN that explicitly uses convolutional layers \url{https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html}} for CIFAR-10 and CelebA. This choice is based on the expectation that, for more complex images, a Convolutional Neural Network (CNN) will perform better than a Multi-Layer Perceptron (MLP). CNNs can learn many optimal filters to apply to the input image for specific tasks, taking advantage of the local connectivity of subsets of pixels since the filters are applied through convolution.

Additionally, a CNN can be viewed as a special type of feed-forward neural network where the weights are constrained. Unlike a feed-forward neural network, the units in the hidden layers of a CNN are not fully connected to the input units. Instead, each unit in the hidden layer is connected to a small region of the input. The weights are shared across the input space because the same filter is applied to each input location. In a feed-forward neural network, each input is connected to each hidden unit with different weights that are not shared and are learned independently (fully connected) \cite{james2023introduction}. However, CNN and pooling layers are more computationally expensive, so for small and simple datasets, using a Multi-Layer Perceptron is a reasonable choice.

\figureWithCaption{cnn_fnn}{Illustration of a FNN and CNN weight sharing from \cite{Winter2018}}{10cm}{0}

Therefore, for MNIST, we chose to implement a Multi-Layer Perceptron due to its simplicity. Each original image in the MNIST dataset contains only a single channel, unlike the other two datasets, which are colored. Additionally, MNIST images are the smallest, with a size of 28x28 pixels.

The specific architecture of our models are specifed in the appendix \ref{appendix:models}.


\section{Launch scripts}
Our distributed MD-GAN implementation allows running multiple nodes either on a single machine or on separate machines. Since each node operates as a separate Python process, they can communicate with each other as long as the network-related parameters are set correctly. To facilitate this, we created two scripts: \code{run-standalone.sh} and \code{run-distributed.sh}. These scripts help start the server and workers in different settings.

By default, we set the generator and discriminator learning rates to 0.0002, $\beta_1$ to 0.5, and $\beta_2$ to 0.999. The number of epochs is set to 3,000, and the batch size $b$ is set to 10. These values are based on the optimal learning rates identified for the models corresponding to the datasets we train on. We chose a small batch size because it can transit over the network more quickly. A larger batch size would take longer per epoch because more data needs to move over the network.

The arguments for each script are defined at the beginning of the script. For the arguments shared by both scripts, we have an additional file named \code{shared-args.sh}, which defines the common parameters across our different experiment settings, ensuring consistency. Detailed descriptions of each argument are provided in the appendix \ref{appendix:args}.

\section{Google Cloud setup}
We received \$200 worth of Google Cloud credits from Prof. Dr. Lydia Chen and her teaching assistant Abel Malan in the context of the Distributed Deep Learning class at the University of Neuchâtel. We used these credits to launch our experiments on two Google Cloud Compute Engine instances. Each instance is equipped with an NVIDIA T4 TPU, 15GB of RAM, and 4 VCPUs. This setup allows us to use the \code{cuda} device to achieve shorter training times and provide faster results.

These two instances communicate with each other through a Virtual Private Cloud, which acts as a local private network. Virtually, both instances are on the same sub-network, enhancing security by avoiding to communicate over Internet. However, it is important to note that even though they are on the same virtual sub-network, they are located in different data centers: one in central Europe and the other in western Europe. This geographic separation is an important factor to consider when analyzing our results.

The network architecture of our application is shown in the figure \ref{fig:architecture}. 



\figureWithCaption{architecture}{Experiments setup. To keep the scheme readable, we simplified the number of black arrows displayed. The three communication arrows represent the connections for all individual workers within the wrapping rectangle.}{12cm}{0}
