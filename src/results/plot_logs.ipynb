{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import plotly.io as pio\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = Path(\"logs\")\n",
    "img_path = Path(\"..\", \"..\", \"report\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_elapsed(df: pd.DataFrame, columns_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    for column_pair in columns_pairs:\n",
    "        start_column, end_column = column_pair\n",
    "        event_name = start_column.replace(\"start.\", \"\")\n",
    "        df[f\"time_elapsed.{event_name}\"] = (df[end_column] - df[start_column]).dt.total_seconds()\n",
    "    return df\n",
    "\n",
    "def convert_all_pairs_to_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for columns in df.columns:\n",
    "        if columns.startswith(\"start\") or columns.startswith(\"end\"):\n",
    "            df[columns] = pd.to_datetime(df[columns], unit=\"s\")\n",
    "    return df\n",
    "\n",
    "def retrieve_start_end_pairs(df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
    "    start_end_pairs: List[Tuple[str, str]] = []\n",
    "    for column in df.columns:\n",
    "        if column.startswith(\"start\"):\n",
    "            start_column = column\n",
    "            end_column = column.replace(\"start\", \"end\")\n",
    "            start_end_pairs.append((start_column, end_column))\n",
    "    return start_end_pairs\n",
    "\n",
    "def dataset_for_every_events(df: pd.DataFrame, columns_pairs: List[Tuple[str, str]], name: str) -> pd.DataFrame:\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    for column_pair in columns_pairs:\n",
    "        start_column, end_column = column_pair\n",
    "        event_name = start_column.replace(\"start.\", \"\")\n",
    "        df_event = df[[start_column, end_column]]\n",
    "        df_event.columns = [\"start\", \"end\"]\n",
    "        df_event = df_event.dropna()\n",
    "        df_event[\"event\"] = event_name\n",
    "        df_event[\"dataset\"] = df[\"dataset\"]\n",
    "        # substract the start time to the first event (datetime object) to get the time elapsed\n",
    "        df_event[\"time_elapsed\"] = (df_event[\"end\"] - df_event[\"start\"]).dt.total_seconds()\n",
    "        df_event[\"name\"] = name\n",
    "        df_event[\"legend\"] = f\"{event_name} - {name}\"\n",
    "        df_event[\"index\"] = df_event.index\n",
    "        dfs.append(df_event)\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def align_start_times(diff_time: float, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for column in df.columns:\n",
    "        if column.startswith(\"start\") or column.startswith(\"end\"):\n",
    "            df[column] = df[column] + pd.Timedelta(seconds=diff_time)\n",
    "    return df\n",
    "\n",
    "def convert_name_to_human_friendly(name: str) -> str:\n",
    "    args = name.split(\".\")\n",
    "    args.pop()\n",
    "    print(args)\n",
    "    if \"worker\" in name:\n",
    "        return f\"Worker {args[-1]} ({args[2]}, {args[1]})\"\n",
    "    if \"server\" in name:\n",
    "        return f\"Server ({args[2]}, {args[1]})\"\n",
    "    if \"standalone\" in name:\n",
    "        return f\"Standalone ({args[0]})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_files = list(logs_path.glob(\"*.*.*.worker.*.logs.csv\"))\n",
    "workers_files.sort(key=lambda x: int(x.stem.split(\".\")[-2]))\n",
    "print(workers_files)\n",
    "\n",
    "workers_events_dfs: List[pd.DataFrame] = []\n",
    "workers_dfs: List[pd.DataFrame] = []\n",
    "for log in workers_files:\n",
    "    dataset = str(log).split(\".\")[-5]\n",
    "    worker = str(log).split(\".\")[-2]\n",
    "    world_size = str(log).split(\".\")[-6]\n",
    "    with open(log) as f:\n",
    "        print(log)\n",
    "        df = pd.read_csv(f)\n",
    "        name = convert_name_to_human_friendly(log.stem)\n",
    "        df[\"dataset\"] = f\"{dataset} ({world_size})\"\n",
    "        df[\"worker\"] = worker\n",
    "        df[\"log\"] = name\n",
    "        df[\"world_size\"] = world_size\n",
    "        df = convert_all_pairs_to_datetime(df)\n",
    "        workers_dfs.append(df)\n",
    "        workers_events_dfs.append(dataset_for_every_events(df, retrieve_start_end_pairs(df), name))\n",
    "workers_df = pd.concat(workers_dfs)\n",
    "workers_events_pairs = retrieve_start_end_pairs(workers_df)\n",
    "print(workers_events_pairs)\n",
    "workers_df = compute_time_elapsed(workers_df, workers_events_pairs)\n",
    "workers_events_df = pd.concat(workers_events_dfs)\n",
    "\n",
    "display(workers_events_df)\n",
    "display(workers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_files = list(logs_path.glob(\"*.*.*.server.logs.csv\"))\n",
    "server_files.sort(key=lambda x: int(x.stem.split(\".\")[1]))\n",
    "print(server_files)\n",
    "\n",
    "server_events_dfs: List[pd.DataFrame] = []\n",
    "server_dfs = []\n",
    "for log in server_files:\n",
    "    dataset = str(log).split(\".\")[-4]\n",
    "    world_size = str(log).split(\".\")[-5]\n",
    "    with open(log) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        name = convert_name_to_human_friendly(log.stem)\n",
    "        df[\"dataset\"] = f\"{dataset} ({world_size})\"\n",
    "        df[\"world_size\"] = world_size\n",
    "        df[\"log\"] = name\n",
    "        df[\"end.recv_data\"] = df[\"start.agg_gradients\"]\n",
    "        df = convert_all_pairs_to_datetime(df)\n",
    "\n",
    "        if len(server_dfs) > 0:\n",
    "            start_time_server: pd.Timedelta = server_dfs[0][\"start.epoch\"].min()\n",
    "            start_time_standalone: pd.Timedelta = df[\"start.epoch\"].min()\n",
    "            diff_time = start_time_server - start_time_standalone\n",
    "            print(f\"diff_time: {diff_time}\")\n",
    "            df = align_start_times(diff_time.total_seconds(), df)\n",
    "\n",
    "        server_dfs.append(df)\n",
    "        server_events_dfs.append(dataset_for_every_events(df, retrieve_start_end_pairs(df), name))\n",
    "server_df = pd.concat(server_dfs)\n",
    "server_events_pairs = retrieve_start_end_pairs(server_df)\n",
    "print(server_events_pairs)\n",
    "server_df = compute_time_elapsed(server_df, server_events_pairs)\n",
    "\n",
    "server_events_df = pd.concat(server_events_dfs)\n",
    "\n",
    "display(server_events_df)\n",
    "display(server_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = server_df[\"size.data\"].iloc[0]\n",
    "feedback_size = server_df[\"size.feedback\"].iloc[0]\n",
    "model_size = workers_df[\"size.model\"].iloc[0]\n",
    "\n",
    "print(f\"Data size: {data_size:.2f}MB\")\n",
    "print(f\"Feedback size: {feedback_size:.2f}MB\")\n",
    "print(f\"Model size: {model_size:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_standalones = list(logs_path.glob(\"*.standalone.logs.csv\"))\n",
    "\n",
    "standalone_dfs = []\n",
    "standalone_events_dfs = []\n",
    "for log in logs_standalones:\n",
    "    dataset = log.stem.split(\".\")[0]\n",
    "    with open(log) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        name = convert_name_to_human_friendly(log.stem)\n",
    "        df[\"dataset\"] = f\"{dataset}\"\n",
    "        df[\"log\"] = name\n",
    "        df = convert_all_pairs_to_datetime(df)\n",
    "\n",
    "        corresponding_server = server_df[(server_df[\"dataset\"] == dataset)]\n",
    "        start_time_server: pd.Timedelta = server_df[\"start.epoch\"].min()\n",
    "        start_time_standalone: pd.Timedelta = df[\"start.epoch\"].min()\n",
    "        diff_time = start_time_server - start_time_standalone\n",
    "        print(f\"diff_time: {diff_time}\")\n",
    "        standalone_df = align_start_times(diff_time.total_seconds(), df)\n",
    "\n",
    "        standalone_dfs.append(df)\n",
    "        standalone_events_dfs.append(dataset_for_every_events(df, retrieve_start_end_pairs(df), name))\n",
    "standalone_df = pd.concat(standalone_dfs)\n",
    "standalone_events_pairs = retrieve_start_end_pairs(standalone_df)\n",
    "print(standalone_events_pairs)\n",
    "standalone_df = compute_time_elapsed(standalone_df, standalone_events_pairs)\n",
    "\n",
    "standalone_events_df = pd.concat(standalone_events_dfs)\n",
    "\n",
    "display(standalone_df)\n",
    "display(standalone_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df = pd.concat([standalone_events_df, workers_events_df, server_events_df])\n",
    "all_df = pd.concat([standalone_df, workers_df, server_df])\n",
    "display(all_events_df)\n",
    "display(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"epoch\": \"Epoch\",\n",
    "    \"mean_d_loss\": \"Mean Discriminator Loss\",\n",
    "    \"mean_g_loss\": \"Mean Generator Loss\",\n",
    "    \"fid\": \"FID\",\n",
    "    \"is\": \"IS\",\n",
    "    \"time_elapsed.epoch_calculation\": \"Epoch duration (s)\",\n",
    "    \"start.epoch\": \"Epoch start time\",\n",
    "    \"log\": \"Actor\",\n",
    "    \"cumsum\": \"Time elapsed (s)\",\n",
    "    \"world_size\": \"Number of workers\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "}\n",
    "rename_columns = {\n",
    "    \"mean_d_loss\": \"Discriminator Loss\",\n",
    "    \"mean_g_loss\": \"Generator Loss\",\n",
    "}\n",
    "standalone_df = standalone_df.rename(columns=rename_columns)\n",
    "all_df = all_df.rename(columns=rename_columns)\n",
    "standalone_server_df = pd.concat([standalone_df, server_df])\n",
    "standalone_server_df = standalone_server_df.rename(columns=rename_columns)\n",
    "\n",
    "display(all_df)\n",
    "display(all_events_df)\n",
    "display(standalone_server_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_white\"\n",
    "WIDTH = 1200\n",
    "HEIGHT = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.iloc[all_df[\"world_size\"].isna().values, all_df.columns.get_loc(\"world_size\")] = 0\n",
    "all_df[\"world_size\"] = all_df[\"world_size\"].astype(int)\n",
    "\n",
    "server_df.iloc[server_df[\"world_size\"].isna().values, server_df.columns.get_loc(\"world_size\")] = 0\n",
    "server_df[\"world_size\"] = server_df[\"world_size\"].astype(int)\n",
    "\n",
    "workers_df.iloc[workers_df[\"world_size\"].isna().values, workers_df.columns.get_loc(\"world_size\")] = 0\n",
    "workers_df[\"world_size\"] = workers_df[\"world_size\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"epoch\": \"Epoch\",\n",
    "    \"size.sent\": \"Size sent (MB)\",\n",
    "    \"size.recv\": \"Size received (MB)\",\n",
    "    \"log\": \"Actor\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "    \"world_size\": \"Number of workers\",\n",
    "}\n",
    "\n",
    "all_df_size = all_df[[\"log\", \"epoch\", \"size.sent\", \"size.recv\", \"dataset\"]]\n",
    "all_df_size = all_df_size.dropna()\n",
    "px.line(all_df_size, x=\"epoch\", y=[\"size.sent\", \"size.recv\"], color=\"log\", title=\"Size sent and received per epoch\", width=WIDTH, height=HEIGHT).show()\n",
    "\n",
    "server_all_sizes = server_df[[\"epoch\", \"size.sent\", \"size.recv\", \"world_size\"]]\n",
    "server_all_sizes = server_all_sizes.groupby([\"world_size\"]).mean().reset_index()\n",
    "server_all_sizes = server_all_sizes.sort_values(by=\"size.sent\")\n",
    "\n",
    "f = px.line(server_all_sizes, x=\"world_size\", y=[\"size.sent\", \"size.recv\"], title=\"Size sent and received per epoch (Server)\", color=\"world_size\", width=WIDTH, height=HEIGHT, labels=labels, markers=True,)\n",
    "f.update_traces(marker=dict(size=10), textposition='top right')\n",
    "for i, d in enumerate(f.data):\n",
    "    for j, a in enumerate(d.x):\n",
    "        f.add_annotation(x=a, y=d.y[j], text=f\"{d['y'][0]:.2f}\", showarrow=False, xshift=-15, yshift=15)\n",
    "f2 = px.line(server_all_sizes, x=\"world_size\", y=[\"size.sent\", \"size.recv\"], title=\"Size sent and received per epoch (Server)\", width=WIDTH, height=HEIGHT, labels=labels)\n",
    "f.add_traces(f2.data)\n",
    "f.write_image(str(img_path / \"size_sent_recv_server.png\"))\n",
    "f.show()\n",
    "\n",
    "workers_all_sizes = workers_df[[\"epoch\", \"size.sent\", \"size.recv\", \"world_size\"]]\n",
    "workers_all_sizes = workers_all_sizes.groupby([\"world_size\"]).mean().reset_index()\n",
    "workers_all_sizes = workers_all_sizes.sort_values(by=\"size.sent\")\n",
    "\n",
    "f = px.line(workers_all_sizes, x=\"world_size\", y=[\"size.sent\", \"size.recv\"], title=\"Size sent and received per epoch (Workers)\", color=\"world_size\", width=WIDTH, height=HEIGHT, labels=labels, markers=True,)\n",
    "f.update_traces(marker=dict(size=10), textposition='top right')\n",
    "for i, d in enumerate(f.data):\n",
    "    for j, a in enumerate(d.x):\n",
    "        f.add_annotation(x=a, y=d.y[j], text=f\"{d['y'][0]:.2f}\", showarrow=False, xshift=-15, yshift=15)\n",
    "f2 = px.line(workers_all_sizes, x=\"world_size\", y=[\"size.sent\", \"size.recv\"], title=\"Size sent and received per epoch (Workers)\", width=WIDTH, height=HEIGHT, labels=labels)\n",
    "f.add_traces(f2.data)\n",
    "f.write_image(str(img_path / \"size_sent_recv_workers.png\"))\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS_CROP = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(standalone_df, x=\"epoch\", y=[\"Discriminator Loss\", \"Generator Loss\"], title=\"Losses standalone\", labels=labels, width=WIDTH, height=HEIGHT).show()\n",
    "px.line(all_df, x=\"epoch\", y=[\"Discriminator Loss\"], color=\"log\", title=\"Losses discriminators\", labels=labels, width=WIDTH, height=HEIGHT).show()\n",
    "\n",
    "selected_log = [\n",
    "    \"Standalone (CIFAR10)\",\n",
    "    \"Server (CIFAR10, 4)\",\n",
    "    \"Server (CIFAR10, 10)\",\n",
    "]\n",
    "\n",
    "fid_df = all_df[[\"epoch\", \"log\", \"fid\"]].dropna()\n",
    "f = px.line(fid_df, x=\"epoch\", y=\"fid\", color=\"log\", title=\"Fréchet Inception Distance\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"fid.png\")\n",
    "f.show()\n",
    "\n",
    "\n",
    "fid_df_10_and_standalone = all_df[[\"epoch\", \"log\", \"fid\"]].dropna()\n",
    "fid_df_10_and_standalone = fid_df_10_and_standalone[fid_df_10_and_standalone[\"log\"].isin([\"Server (CIFAR10, 10)\", \"Standalone (CIFAR10)\"])]\n",
    "f = px.line(fid_df_10_and_standalone, x=\"epoch\", y=\"fid\", color=\"log\", title=\"Fréchet Inception Distance\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"fid_4_10_standalone.png\")\n",
    "f.show()\n",
    "\n",
    "fid_df_cropped = fid_df[fid_df[\"epoch\"] < MAX_EPOCHS_CROP]\n",
    "f = px.line(fid_df_cropped, x=\"epoch\", y=\"fid\", color=\"log\", title=\"Fréchet Inception Distance\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"fid_cropped.png\")\n",
    "f.show()\n",
    "\n",
    "fid_df = fid_df[fid_df[\"log\"].isin(selected_log)]\n",
    "fid_df = fid_df[fid_df[\"epoch\"] % 900 == 0]\n",
    "f = px.line(fid_df, x=\"epoch\", y=\"fid\", color=\"log\", title=\"Fréchet Inception Distance\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"fid_filtered.png\")\n",
    "f.show()\n",
    "\n",
    "is_df = all_df[[\"epoch\", \"log\", \"is\"]].dropna()\n",
    "f = px.line(is_df, x=\"epoch\", y=\"is\", color=\"log\", title=\"Inception Score\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"is.png\")\n",
    "f.show()\n",
    "\n",
    "is_df_10_and_standalone = all_df[[\"epoch\", \"log\", \"is\"]].dropna()\n",
    "is_df_10_and_standalone = is_df_10_and_standalone[is_df_10_and_standalone[\"log\"].isin([\"Server (CIFAR10, 10)\", \"Standalone (CIFAR10)\"])]\n",
    "f = px.line(is_df_10_and_standalone, x=\"epoch\", y=\"is\", color=\"log\", title=\"Inception Score\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"is_4_10_standalone.png\")\n",
    "f.show()\n",
    "\n",
    "is_df_cropped = is_df[is_df[\"epoch\"] < MAX_EPOCHS_CROP]\n",
    "f = px.line(is_df_cropped, x=\"epoch\", y=\"is\", color=\"log\", title=\"Inception Score\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"is_cropped.png\")\n",
    "f.show()\n",
    "\n",
    "is_df = is_df[is_df[\"log\"].isin(selected_log)]\n",
    "is_df = is_df[is_df[\"epoch\"] % 900 == 0]\n",
    "f = px.line(is_df, x=\"epoch\", y=\"is\", color=\"log\", title=\"Inception Score\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"is_filtered.png\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_epoch_calculation_standalone = standalone_df[\"time_elapsed.epoch_calculation\"].mean()\n",
    "print(f\"Average time epoch calculation standalone: {avg_time_epoch_calculation_standalone:.5f}s\")\n",
    "\n",
    "standalone_df_without_outliers_dfs = []\n",
    "avg_time_per_epoch = {\n",
    "    \"Standalone\": avg_time_epoch_calculation_standalone,\n",
    "}\n",
    "world_sizes = [0]\n",
    "for server in server_df[\"log\"].unique():\n",
    "    print()\n",
    "    df = server_df[server_df[\"log\"] == server].copy()\n",
    "    world_sizes.append(int(df[\"world_size\"].iloc[0]))\n",
    "    # remove outliers\n",
    "    df = df[df[\"time_elapsed.epoch_calculation\"] < df[\"time_elapsed.epoch_calculation\"].quantile(0.95)]\n",
    "    df = df[df[\"time_elapsed.epoch_calculation\"] > df[\"time_elapsed.epoch_calculation\"].quantile(0.05)]\n",
    "    standalone_df_without_outliers_dfs.append(df)\n",
    "\n",
    "    avg_time_epoch_calculation_server = df[\"time_elapsed.epoch_calculation\"].mean()\n",
    "    print(f\"Average time epoch calculation server ({server}): {avg_time_epoch_calculation_server:.5f}s\")\n",
    "    ratio = avg_time_epoch_calculation_server / avg_time_epoch_calculation_standalone\n",
    "    print(f\"Ratio: {ratio:.2f}, the server {server} is {ratio:.2f} times slower than the standalone\")\n",
    "    avg_time_per_epoch[server] = avg_time_epoch_calculation_server\n",
    "standalone_df_without_outliers = pd.concat([standalone_df, *standalone_df_without_outliers_dfs])\n",
    "df_avg_time_per_epoch = pd.DataFrame(avg_time_per_epoch.items(), columns=[\"Actor\", \"Average time per epoch (s)\"])\n",
    "df_avg_time_per_epoch[\"world_size\"] = world_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in standalone_server_df[\"log\"].unique():\n",
    "    cum_sum = standalone_server_df[standalone_server_df[\"log\"] == log][\"time_elapsed.epoch_calculation\"].cumsum()\n",
    "    standalone_server_df.loc[standalone_server_df[\"log\"] == log, \"cumsum\"] = cum_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_time_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = px.line(standalone_server_df, x=\"epoch\", y=\"time_elapsed.epoch_calculation\", color=\"log\", title=\"Epoch duration\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_duration.png\")\n",
    "f.show()\n",
    "\n",
    "standalone_server_df_cropped = standalone_server_df[standalone_server_df[\"epoch\"] <= MAX_EPOCHS_CROP]\n",
    "f = px.line(standalone_server_df_cropped, x=\"epoch\", y=\"time_elapsed.epoch_calculation\", color=\"log\", title=\"Epoch duration\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_duration_cropped.png\")\n",
    "f.show()\n",
    "\n",
    "f = px.line(standalone_server_df, x=\"epoch\", y=\"cumsum\", color=\"log\", title=\"Epoch achieve as the time pass\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_start_time.png\")\n",
    "f.show()\n",
    "\n",
    "standalone_server_df_cropped = standalone_server_df[standalone_server_df[\"epoch\"] <= MAX_EPOCHS_CROP]\n",
    "f = px.line(standalone_server_df_cropped, x=\"epoch\", y=\"cumsum\", color=\"log\", title=\"Epoch achieve as the time pass\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_start_time_cropped.png\")\n",
    "f.show()\n",
    "\n",
    "f = px.line(standalone_df_without_outliers, x=\"epoch\", y=\"time_elapsed.epoch_calculation\", color=\"log\", title=\"Epoch duration (without outliers)\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_duration_without_outliers.png\")\n",
    "f.show()\n",
    "\n",
    "standalone_df_without_outliers_cropped = standalone_df_without_outliers[standalone_df_without_outliers[\"epoch\"] <= MAX_EPOCHS_CROP]\n",
    "f = px.line(standalone_df_without_outliers_cropped, x=\"epoch\", y=\"time_elapsed.epoch_calculation\", color=\"log\", title=\"Epoch duration (without outliers)\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "f.write_image(img_path / \"epoch_duration_without_outliers_cropped.png\")\n",
    "f.show()\n",
    "\n",
    "f = px.bar(df_avg_time_per_epoch, x=\"Actor\", y=\"Average time per epoch (s)\", title=\"Average time per epoch (s)\", text=df_avg_time_per_epoch[\"Average time per epoch (s)\"].apply(lambda x: f\"{x:.2f}s\"), width=WIDTH, height=HEIGHT, text_auto=True, color=\"Actor\")\n",
    "f.write_image(img_path / \"average_time_per_epoch.png\")\n",
    "f.show()\n",
    "\n",
    "f = px.line(df_avg_time_per_epoch, x=\"world_size\", y=\"Average time per epoch (s)\", text=df_avg_time_per_epoch[\"Average time per epoch (s)\"].apply(lambda x: f\"{x:.2f}s\"), color=\"Actor\", title=\"Average time per epoch (s)\", labels=labels, width=WIDTH, height=HEIGHT, markers=True)\n",
    "f.update_traces(marker=dict(size=10), textposition='top right')\n",
    "f2 = px.line(df_avg_time_per_epoch, x=\"world_size\", y=\"Average time per epoch (s)\", title=\"Average time per epoch (s)\", width=WIDTH, height=HEIGHT)\n",
    "f2.data[0].update(opacity=0.5)\n",
    "f.add_trace(f2.data[0])\n",
    "f.write_image(img_path / \"average_time_per_epoch_relation.png\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"generate_data\",\n",
    "    \"discriminator_train\",\n",
    "    \"generator_train\",\n",
    "]\n",
    "rename_events = {\n",
    "    \"generate_data\": \"Generate data\",\n",
    "    \"calc_gradients\": \"Perform optimization step\",\n",
    "    \"discriminator_train\": \"Perform an optimization step on the discriminator\",\n",
    "    \"generator_train\": \"Perform an optimization step on the generator\",\n",
    "}\n",
    "labels = {\n",
    "    \"time_elapsed\": \"Average time elapsed (s)\",\n",
    "    \"event\": \"Event\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "}\n",
    "\n",
    "mean_time_elapsed = standalone_events_df[[\"event\", \"dataset\", \"time_elapsed\"]].groupby([\"event\", \"dataset\"]).mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", color=\"dataset\", title=\"Mean time elapsed per operations (Server)\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "f.write_image(img_path / \"mean_time_elapsed_bar_standalone.png\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"generate_data\",\n",
    "    \"send_data\",\n",
    "    \"recv_data\",\n",
    "    \"agg_gradients\",\n",
    "    \"calc_gradients\",\n",
    "    \"swap\"\n",
    "]\n",
    "rename_events = {\n",
    "    \"generate_data\": \"Generate data\",\n",
    "    \"send_data\": \"Send data to workers\",\n",
    "    \"recv_data\": \"Receive data from workers\",\n",
    "    \"agg_gradients\": \"Aggregate gradients\",\n",
    "    \"calc_gradients\": \"Perform optimization step\",\n",
    "    \"swap\": \"Send swap instruction to workers\",\n",
    "}\n",
    "labels = {\n",
    "    \"time_elapsed\": \"Average time elapsed (s)\",\n",
    "    \"event\": \"Event\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "}\n",
    "\n",
    "# for server in server_events_df[\"name\"].unique():\n",
    "#     df = server_events_df[server_events_df[\"name\"] == server]\n",
    "#     mean_time_elapsed = df[[\"event\", \"time_elapsed\"]].groupby(\"event\").mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "#     mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "#     mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "    \n",
    "#     f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", title=f\"Mean time elapsed {server}\", color=\"event\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "#     f.write_image(img_path / f\"mean_time_elapsed_bar_{server}.png\")\n",
    "#     f.show()\n",
    "\n",
    "#     f = px.pie(mean_time_elapsed, values=\"time_elapsed\", names=\"event\", title=f\"Mean time elapsed {server}\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "#     f.write_image(img_path / f\"mean_time_elapsed_pie_{server}.png\")\n",
    "#     f.show()\n",
    "outliers_server_epochs_dfs = []\n",
    "normal_server_epochs_dfs = []\n",
    "for server in server_events_df[\"name\"].unique():\n",
    "    df = server_events_df[server_events_df[\"name\"] == server]\n",
    "    # identify the epochs where event epoch_calculation is an outlier\n",
    "    event_df = df[df[\"event\"] == \"epoch_calculation\"]\n",
    "    outliers_epochs = event_df[(event_df[\"time_elapsed\"] < event_df[\"time_elapsed\"].quantile(0.05)) | (event_df[\"time_elapsed\"] > event_df[\"time_elapsed\"].quantile(0.95))][\"index\"]\n",
    "    outliers_server_epochs_dfs.append(df[df[\"index\"].isin(outliers_epochs)])\n",
    "    normal_server_epochs_dfs.append(df[~df[\"index\"].isin(outliers_epochs)])\n",
    "outliers_server_epochs_df = pd.concat(outliers_server_epochs_dfs)\n",
    "normal_server_epochs_df = pd.concat(normal_server_epochs_dfs)\n",
    "\n",
    "mean_time_elapsed = normal_server_epochs_df[[\"event\", \"dataset\", \"time_elapsed\"]].groupby([\"event\", \"dataset\"]).mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", color=\"dataset\", title=\"Mean time elapsed per operations (Server)\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "f.write_image(img_path / \"mean_time_elapsed_bar_server.png\")\n",
    "f.show()\n",
    "\n",
    "mean_time_elapsed = outliers_server_epochs_df[[\"event\", \"dataset\", \"time_elapsed\"]].groupby([\"event\", \"dataset\"]).mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", color=\"dataset\", title=\"Outliers - mean time elapsed per operations (Server)\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "f.write_image(img_path / \"mean_time_elapsed_bar_server_outliers.png\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"recv_data\",\n",
    "    \"calc_gradients\",\n",
    "    \"send\",\n",
    "    \"swap_recv_instructions\",\n",
    "    \"swap_send\",\n",
    "    \"swap_recv\",\n",
    "    \"load_state_dict\",\n",
    "]\n",
    "rename_events = {\n",
    "    \"recv_data\": \"Receive data from server\",\n",
    "    \"calc_gradients\": \"Perform optimization step\",\n",
    "    \"send\": \"Send gradients to server\",\n",
    "    \"swap_recv_instructions\": \"Receive swap instructions from server\",\n",
    "    \"swap_send\": \"Send discriminator weights to other worker\",\n",
    "    \"swap_recv\": \"Receive discriminator weights from other worker\",\n",
    "    \"load_state_dict\": \"Load received model (swap)\",\n",
    "}\n",
    "labels = {\n",
    "    \"time_elapsed\": \"Average time elapsed (s)\",\n",
    "    \"event\": \"Event\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "}\n",
    "# for worker in workers_events_df[\"dataset\"].unique():\n",
    "#     dataset, world_size = worker.split(\" \")\n",
    "#     world_size = int(world_size.replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "#     df = workers_events_df[workers_events_df[\"dataset\"] == worker]\n",
    "#     mean_time_elapsed = df[[\"event\", \"time_elapsed\"]].groupby(\"event\").mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "#     mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "#     mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "\n",
    "#     f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", title=f\"Mean time elapsed workers ({dataset}, {world_size})\", color=\"event\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "#     f.write_image(img_path / f\"mean_time_elapsed_bar_workers_{dataset}_{world_size}.png\")\n",
    "#     f.show()\n",
    "\n",
    "#     f = px.pie(mean_time_elapsed, values=\"time_elapsed\", names=\"event\", title=f\"Mean time elapsed workers ({dataset}, {world_size})\", labels=labels, width=WIDTH, height=HEIGHT)\n",
    "#     f.write_image(img_path / f\"mean_time_elapsed_bar_workers_{dataset}_{world_size}.png\")\n",
    "#     f.show()\n",
    "    \n",
    "#     display(mean_time_elapsed)\n",
    "outliers_workers_epochs_dfs = []\n",
    "normal_workers_epochs_dfs = []\n",
    "for worker in workers_events_df[\"dataset\"].unique():\n",
    "    df = workers_events_df[workers_events_df[\"dataset\"] == worker]\n",
    "    # identify the epochs where event epoch_calculation is an outlier\n",
    "    event_df = df[df[\"event\"] == \"epoch\"]\n",
    "    outliers_epochs = event_df[(event_df[\"time_elapsed\"] < event_df[\"time_elapsed\"].quantile(0.05)) | (event_df[\"time_elapsed\"] > event_df[\"time_elapsed\"].quantile(0.95))][\"index\"]\n",
    "    outliers_workers_epochs_dfs.append(df[df[\"index\"].isin(outliers_epochs)])\n",
    "    normal_workers_epochs_dfs.append(df[~df[\"index\"].isin(outliers_epochs)])\n",
    "outliers_workers_epochs_df = pd.concat(outliers_workers_epochs_dfs)\n",
    "normal_workers_epochs_df = pd.concat(normal_workers_epochs_dfs)\n",
    "\n",
    "mean_time_elapsed = normal_workers_epochs_df[[\"event\", \"dataset\", \"time_elapsed\"]].groupby([\"event\", \"dataset\"]).mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", title=f\"Mean time elapsed per operations (Workers)\", color=\"dataset\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "f.write_image(img_path / f\"mean_time_elapsed_bar_workers.png\")\n",
    "f.show()\n",
    "\n",
    "mean_time_elapsed = outliers_workers_epochs_df[[\"event\", \"dataset\", \"time_elapsed\"]].groupby([\"event\", \"dataset\"]).mean().sort_values(by=\"time_elapsed\").reset_index()\n",
    "mean_time_elapsed = mean_time_elapsed[mean_time_elapsed[\"event\"].isin(selected_columns)].reset_index(drop=True)\n",
    "mean_time_elapsed[\"event\"] = mean_time_elapsed[\"event\"].replace(rename_events)\n",
    "f = px.bar(mean_time_elapsed, x=\"event\", y=\"time_elapsed\", color=\"dataset\", title=\"Outliers - mean time elapsed per operations (Worker)\", labels=labels, width=WIDTH, height=HEIGHT, text_auto=True)\n",
    "f.write_image(img_path / \"mean_time_elapsed_bar_workers_outliers.png\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 10\n",
    "events = [\n",
    "    \"generate_data\",\n",
    "    \"send_data\",\n",
    "    \"recv_data\",\n",
    "    \"calc_gradients\",\n",
    "    \"apply_gradients\",\n",
    "    \"swap\",\n",
    "    \"fid\",\n",
    "    \"is\",\n",
    "    \"send\",\n",
    "]\n",
    "rename_events = {\n",
    "    \"generate_data\": \"(Standalone/Server) Generate data\",\n",
    "    \"send_data\": \"(Server) Send data to workers\",\n",
    "    \"recv_data\": \"(Server/Worker) Receive feedbacks/data from workers/server\",\n",
    "    \"apply_gradients\": \"(Server) Perform optimization step\",\n",
    "    \"swap\": \"Send swap instruction to workers\",\n",
    "    \"calc_gradients\": \"(Standalone/Worker) Perform optimization step / (Server) Aggregate gradients\",\n",
    "    \"send\": \"(Worker) Send gradients to server\",\n",
    "    \"fid\": \"(Standalone/Server) Calculate FID\",\n",
    "    \"is\": \"(Standalone/Server) Calculate IS\",\n",
    "}\n",
    "labels = {\n",
    "    \"event\": \"Event\",\n",
    "}\n",
    "selected_dataset = \"CIFAR10 (4)\"\n",
    "all_events_df_timeline = all_events_df[all_events_df[\"event\"].isin(events)].copy()\n",
    "all_events_df_timeline[\"event\"] = all_events_df_timeline[\"event\"].replace(rename_events)\n",
    "all_events_df_timeline = all_events_df_timeline[all_events_df_timeline[\"dataset\"] == selected_dataset]\n",
    "\n",
    "timeline_start = px.timeline(\n",
    "    all_events_df_timeline[(all_events_df_timeline[\"index\"] > 10) & (all_events_df_timeline[\"index\"] <= 10 + MAX_EPOCHS)],\n",
    "    x_start=\"start\",\n",
    "    x_end=\"end\",\n",
    "    color=\"name\",\n",
    "    y=\"event\",\n",
    "    opacity=0.5,\n",
    "    template=\"plotly_white\",\n",
    "    labels=labels,\n",
    "    width=WIDTH*2,\n",
    "    height=HEIGHT,\n",
    "    title=f\"Timeline of events for {selected_dataset} - {MAX_EPOCHS} epochs\"\n",
    ")\n",
    "timeline_start.write_image(img_path / \"timeline_10.png\")\n",
    "timeline_start.show()\n",
    "\n",
    "\n",
    "timeline_start = px.timeline(\n",
    "    all_events_df_timeline[(all_events_df_timeline[\"index\"] > 10) & (all_events_df_timeline[\"index\"] <= 10 + 1)],\n",
    "    x_start=\"start\",\n",
    "    x_end=\"end\",\n",
    "    color=\"name\",\n",
    "    y=\"event\",\n",
    "    opacity=0.5,\n",
    "    template=\"plotly_white\",\n",
    "    labels=labels,\n",
    "    width=WIDTH*2,\n",
    "    height=HEIGHT,\n",
    "    title=f\"Timeline of events for {selected_dataset} - one epoch\"\n",
    ")\n",
    "timeline_start.write_image(img_path / \"timeline_1.png\")\n",
    "timeline_start.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
